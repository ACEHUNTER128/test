{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSqr90byg7CMXeXxY/ECSH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ACEHUNTER128/test/blob/main/AdvancedProgramming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 01: Tensoflow:\n",
        "Task 01:\n",
        "\n"
      ],
      "metadata": {
        "id": "QomK-sAQSv2U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "ukxiYeQiR0z2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize\n",
        "y_train, y_test = to_categorical(y_train), to_categorical(y_test)"
      ],
      "metadata": {
        "id": "VYv68d2ASD71"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bttP5WhiSaeM",
        "outputId": "e823d7ca-0b98-4420-931a-a4e34168a0fc"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvEpW6bISgqq",
        "outputId": "07337fe0-45a9-4c2c-8d09-570d427495b1"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.8600 - loss: 0.4998\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9527 - loss: 0.1641\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.9671 - loss: 0.1128\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9750 - loss: 0.0845\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.9794 - loss: 0.0683\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7bc6d70849d0>"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 01: Task 02"
      ],
      "metadata": {
        "id": "9Vs3RRhF5aey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying Task 2 in the Tensor Flow Part\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy (TF): {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3Uqmced5VkP",
        "outputId": "5afc7d0a-0dec-47c6-f23e-a6fff2d76693"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9697 - loss: 0.1090\n",
            "Test Accuracy (TF): 0.9724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 01: Task 3"
      ],
      "metadata": {
        "id": "--GcYKDv5Jls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 3: TensorFlow conversion to TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "with open(\"model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrsxXevef4pJ",
        "outputId": "67dfd1c2-fb80-4050-a9f4-b74ebe0de69e"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpxe7yvvii'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor_16')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  136094062725072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136094062732368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136094062734288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136094062732176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2: PyTorch :\n",
        "Task 01:"
      ],
      "metadata": {
        "id": "U3X0C8kPSntt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "tjErrIJLStDp"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Scale to [-1, 1]\n",
        "])\n",
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "QHqS0HBMcz7m"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)  # Flatten\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)  # Logits (no softmax for CrossEntropyLoss)\n",
        "\n",
        "model = Net()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "JSMIMHEEc5O-"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if batch_idx % 100 == 99:\n",
        "            print(f\"Epoch {epoch+1}, Batch {batch_idx+1}: Loss = {running_loss / 100:.4f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "# Save the trained model for Task 2\n",
        "torch.save(model.state_dict(), 'mnist_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNLwB5PJdBQp",
        "outputId": "2f2e4794-e756-45e6-aa9e-196149d42807"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100: Loss = 1.1705\n",
            "Epoch 1, Batch 200: Loss = 0.5487\n",
            "Epoch 1, Batch 300: Loss = 0.4555\n",
            "Epoch 1, Batch 400: Loss = 0.4091\n",
            "Epoch 1, Batch 500: Loss = 0.3844\n",
            "Epoch 1, Batch 600: Loss = 0.3675\n",
            "Epoch 1, Batch 700: Loss = 0.3433\n",
            "Epoch 1, Batch 800: Loss = 0.3565\n",
            "Epoch 1, Batch 900: Loss = 0.3324\n",
            "Epoch 1, Batch 1000: Loss = 0.2912\n",
            "Epoch 1, Batch 1100: Loss = 0.3154\n",
            "Epoch 1, Batch 1200: Loss = 0.3180\n",
            "Epoch 1, Batch 1300: Loss = 0.3050\n",
            "Epoch 1, Batch 1400: Loss = 0.3162\n",
            "Epoch 1, Batch 1500: Loss = 0.2904\n",
            "Epoch 1, Batch 1600: Loss = 0.2658\n",
            "Epoch 1, Batch 1700: Loss = 0.2900\n",
            "Epoch 1, Batch 1800: Loss = 0.2623\n",
            "Epoch 2, Batch 100: Loss = 0.2826\n",
            "Epoch 2, Batch 200: Loss = 0.2310\n",
            "Epoch 2, Batch 300: Loss = 0.2447\n",
            "Epoch 2, Batch 400: Loss = 0.1982\n",
            "Epoch 2, Batch 500: Loss = 0.2267\n",
            "Epoch 2, Batch 600: Loss = 0.2306\n",
            "Epoch 2, Batch 700: Loss = 0.2010\n",
            "Epoch 2, Batch 800: Loss = 0.2138\n",
            "Epoch 2, Batch 900: Loss = 0.2031\n",
            "Epoch 2, Batch 1000: Loss = 0.2047\n",
            "Epoch 2, Batch 1100: Loss = 0.1981\n",
            "Epoch 2, Batch 1200: Loss = 0.2309\n",
            "Epoch 2, Batch 1300: Loss = 0.2275\n",
            "Epoch 2, Batch 1400: Loss = 0.1942\n",
            "Epoch 2, Batch 1500: Loss = 0.1834\n",
            "Epoch 2, Batch 1600: Loss = 0.2088\n",
            "Epoch 2, Batch 1700: Loss = 0.1880\n",
            "Epoch 2, Batch 1800: Loss = 0.1855\n",
            "Epoch 3, Batch 100: Loss = 0.1513\n",
            "Epoch 3, Batch 200: Loss = 0.1622\n",
            "Epoch 3, Batch 300: Loss = 0.1724\n",
            "Epoch 3, Batch 400: Loss = 0.1540\n",
            "Epoch 3, Batch 500: Loss = 0.1677\n",
            "Epoch 3, Batch 600: Loss = 0.1736\n",
            "Epoch 3, Batch 700: Loss = 0.1715\n",
            "Epoch 3, Batch 800: Loss = 0.1649\n",
            "Epoch 3, Batch 900: Loss = 0.1576\n",
            "Epoch 3, Batch 1000: Loss = 0.1771\n",
            "Epoch 3, Batch 1100: Loss = 0.1648\n",
            "Epoch 3, Batch 1200: Loss = 0.1717\n",
            "Epoch 3, Batch 1300: Loss = 0.1694\n",
            "Epoch 3, Batch 1400: Loss = 0.1742\n",
            "Epoch 3, Batch 1500: Loss = 0.1618\n",
            "Epoch 3, Batch 1600: Loss = 0.1515\n",
            "Epoch 3, Batch 1700: Loss = 0.1675\n",
            "Epoch 3, Batch 1800: Loss = 0.1645\n",
            "Epoch 4, Batch 100: Loss = 0.1198\n",
            "Epoch 4, Batch 200: Loss = 0.1266\n",
            "Epoch 4, Batch 300: Loss = 0.1343\n",
            "Epoch 4, Batch 400: Loss = 0.1349\n",
            "Epoch 4, Batch 500: Loss = 0.1513\n",
            "Epoch 4, Batch 600: Loss = 0.1497\n",
            "Epoch 4, Batch 700: Loss = 0.1343\n",
            "Epoch 4, Batch 800: Loss = 0.1425\n",
            "Epoch 4, Batch 900: Loss = 0.1575\n",
            "Epoch 4, Batch 1000: Loss = 0.1274\n",
            "Epoch 4, Batch 1100: Loss = 0.1114\n",
            "Epoch 4, Batch 1200: Loss = 0.1363\n",
            "Epoch 4, Batch 1300: Loss = 0.1475\n",
            "Epoch 4, Batch 1400: Loss = 0.1306\n",
            "Epoch 4, Batch 1500: Loss = 0.1324\n",
            "Epoch 4, Batch 1600: Loss = 0.1310\n",
            "Epoch 4, Batch 1700: Loss = 0.1487\n",
            "Epoch 4, Batch 1800: Loss = 0.1478\n",
            "Epoch 5, Batch 100: Loss = 0.1228\n",
            "Epoch 5, Batch 200: Loss = 0.1175\n",
            "Epoch 5, Batch 300: Loss = 0.1242\n",
            "Epoch 5, Batch 400: Loss = 0.1236\n",
            "Epoch 5, Batch 500: Loss = 0.1159\n",
            "Epoch 5, Batch 600: Loss = 0.1292\n",
            "Epoch 5, Batch 700: Loss = 0.1259\n",
            "Epoch 5, Batch 800: Loss = 0.1370\n",
            "Epoch 5, Batch 900: Loss = 0.1190\n",
            "Epoch 5, Batch 1000: Loss = 0.1249\n",
            "Epoch 5, Batch 1100: Loss = 0.1089\n",
            "Epoch 5, Batch 1200: Loss = 0.1234\n",
            "Epoch 5, Batch 1300: Loss = 0.1198\n",
            "Epoch 5, Batch 1400: Loss = 0.1291\n",
            "Epoch 5, Batch 1500: Loss = 0.1129\n",
            "Epoch 5, Batch 1600: Loss = 0.1239\n",
            "Epoch 5, Batch 1700: Loss = 0.1067\n",
            "Epoch 5, Batch 1800: Loss = 0.1260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 02: Task 02"
      ],
      "metadata": {
        "id": "yJT62KRBd8Gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "ZE7xNYJfdcz5"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test data (same preprocessing as Task 1)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "test_data = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)"
      ],
      "metadata": {
        "id": "R_sOmmUtdgOx"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuild the model architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n"
      ],
      "metadata": {
        "id": "XWZyIc0mdle4"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model from Task 1\n",
        "model = Net()\n",
        "model.load_state_dict(torch.load('mnist_model.pth'))\n",
        "model.eval()  # Set to evaluation mode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1A_llGGdvgb",
        "outputId": "c677c5e8-1a0d-4483-a48a-0ff8864d5309"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += pred.eq(target).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {correct / len(test_loader.dataset):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yf8Wns3VdzEz",
        "outputId": "61a72533-4b86-4ff2-e93c-000be93d0921"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 02: Task 03"
      ],
      "metadata": {
        "id": "UCJqDySG47H3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c9e6b62",
        "outputId": "52892992-1ff4-4793-bc36-2e56781650c0"
      },
      "source": [
        "%pip install onnx"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (1.18.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 3 Conversion to ONNX\n",
        "import onnx\n",
        "dummy_input = torch.randn(1, 1, 28, 28)  # Batch of 1 image\n",
        "torch.onnx.export(model, dummy_input, \"model.onnx\",\n",
        "                  input_names=[\"input\"], output_names=[\"output\"])"
      ],
      "metadata": {
        "id": "cslyTa3ThPmd"
      },
      "execution_count": 96,
      "outputs": []
    }
  ]
}