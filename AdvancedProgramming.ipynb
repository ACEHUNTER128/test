{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhgbidHRaVRxH54YejUD5r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ACEHUNTER128/test/blob/main/AdvancedProgramming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 01: TensorFlow (with the Task 2 included in)"
      ],
      "metadata": {
        "id": "QomK-sAQSv2U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ukxiYeQiR0z2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize\n",
        "y_train, y_test = to_categorical(y_train), to_categorical(y_test)"
      ],
      "metadata": {
        "id": "VYv68d2ASD71"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bttP5WhiSaeM",
        "outputId": "2c044b70-bfda-4e18-92d1-5d9b9eb1d6fc"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
        "\n",
        "#Applying Task 2 in the Tensor Flow Part\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy (TF): {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvEpW6bISgqq",
        "outputId": "52247a27-28f8-4c2e-819a-75d66b931238"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8573 - loss: 0.4962\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9555 - loss: 0.1584\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9667 - loss: 0.1139\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9750 - loss: 0.0843\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9796 - loss: 0.0688\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9663 - loss: 0.1127\n",
            "Test Accuracy (TF): 0.9720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 3: TensorFlow conversion to TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "with open(\"model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrsxXevef4pJ",
        "outputId": "3e614732-127b-417d-f2c9-57a884c4797b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpmfdxad2s'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor_8')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  136090200470352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136090200460560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136090200470160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136090196817104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2: PyTorch"
      ],
      "metadata": {
        "id": "U3X0C8kPSntt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "tjErrIJLStDp"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Scale to [-1, 1]\n",
        "])\n",
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "QHqS0HBMcz7m"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)  # Flatten\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)  # Logits (no softmax for CrossEntropyLoss)\n",
        "\n",
        "model = Net()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "JSMIMHEEc5O-"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if batch_idx % 100 == 99:\n",
        "            print(f\"Epoch {epoch+1}, Batch {batch_idx+1}: Loss = {running_loss / 100:.4f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "# Save the trained model for Task 2\n",
        "torch.save(model.state_dict(), 'mnist_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNLwB5PJdBQp",
        "outputId": "22137771-2c3a-46b9-a336-3beca4cc3953"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100: Loss = 1.1978\n",
            "Epoch 1, Batch 200: Loss = 0.5206\n",
            "Epoch 1, Batch 300: Loss = 0.4783\n",
            "Epoch 1, Batch 400: Loss = 0.4010\n",
            "Epoch 1, Batch 500: Loss = 0.3770\n",
            "Epoch 1, Batch 600: Loss = 0.3797\n",
            "Epoch 1, Batch 700: Loss = 0.3491\n",
            "Epoch 1, Batch 800: Loss = 0.3235\n",
            "Epoch 1, Batch 900: Loss = 0.3446\n",
            "Epoch 1, Batch 1000: Loss = 0.3153\n",
            "Epoch 1, Batch 1100: Loss = 0.3400\n",
            "Epoch 1, Batch 1200: Loss = 0.3186\n",
            "Epoch 1, Batch 1300: Loss = 0.3376\n",
            "Epoch 1, Batch 1400: Loss = 0.2960\n",
            "Epoch 1, Batch 1500: Loss = 0.2873\n",
            "Epoch 1, Batch 1600: Loss = 0.2934\n",
            "Epoch 1, Batch 1700: Loss = 0.2930\n",
            "Epoch 1, Batch 1800: Loss = 0.2656\n",
            "Epoch 2, Batch 100: Loss = 0.2533\n",
            "Epoch 2, Batch 200: Loss = 0.2555\n",
            "Epoch 2, Batch 300: Loss = 0.2441\n",
            "Epoch 2, Batch 400: Loss = 0.2487\n",
            "Epoch 2, Batch 500: Loss = 0.2336\n",
            "Epoch 2, Batch 600: Loss = 0.2588\n",
            "Epoch 2, Batch 700: Loss = 0.2304\n",
            "Epoch 2, Batch 800: Loss = 0.2240\n",
            "Epoch 2, Batch 900: Loss = 0.2438\n",
            "Epoch 2, Batch 1000: Loss = 0.2246\n",
            "Epoch 2, Batch 1100: Loss = 0.2248\n",
            "Epoch 2, Batch 1200: Loss = 0.2110\n",
            "Epoch 2, Batch 1300: Loss = 0.1946\n",
            "Epoch 2, Batch 1400: Loss = 0.2067\n",
            "Epoch 2, Batch 1500: Loss = 0.1843\n",
            "Epoch 2, Batch 1600: Loss = 0.1860\n",
            "Epoch 2, Batch 1700: Loss = 0.1819\n",
            "Epoch 2, Batch 1800: Loss = 0.1546\n",
            "Epoch 3, Batch 100: Loss = 0.1531\n",
            "Epoch 3, Batch 200: Loss = 0.2045\n",
            "Epoch 3, Batch 300: Loss = 0.1808\n",
            "Epoch 3, Batch 400: Loss = 0.1756\n",
            "Epoch 3, Batch 500: Loss = 0.1705\n",
            "Epoch 3, Batch 600: Loss = 0.1743\n",
            "Epoch 3, Batch 700: Loss = 0.1679\n",
            "Epoch 3, Batch 800: Loss = 0.1818\n",
            "Epoch 3, Batch 900: Loss = 0.1773\n",
            "Epoch 3, Batch 1000: Loss = 0.1571\n",
            "Epoch 3, Batch 1100: Loss = 0.1722\n",
            "Epoch 3, Batch 1200: Loss = 0.1676\n",
            "Epoch 3, Batch 1300: Loss = 0.1595\n",
            "Epoch 3, Batch 1400: Loss = 0.1559\n",
            "Epoch 3, Batch 1500: Loss = 0.1525\n",
            "Epoch 3, Batch 1600: Loss = 0.1586\n",
            "Epoch 3, Batch 1700: Loss = 0.1773\n",
            "Epoch 3, Batch 1800: Loss = 0.2069\n",
            "Epoch 4, Batch 100: Loss = 0.1569\n",
            "Epoch 4, Batch 200: Loss = 0.1486\n",
            "Epoch 4, Batch 300: Loss = 0.1530\n",
            "Epoch 4, Batch 400: Loss = 0.1275\n",
            "Epoch 4, Batch 500: Loss = 0.1529\n",
            "Epoch 4, Batch 600: Loss = 0.1144\n",
            "Epoch 4, Batch 700: Loss = 0.1512\n",
            "Epoch 4, Batch 800: Loss = 0.1278\n",
            "Epoch 4, Batch 900: Loss = 0.1378\n",
            "Epoch 4, Batch 1000: Loss = 0.1610\n",
            "Epoch 4, Batch 1100: Loss = 0.1663\n",
            "Epoch 4, Batch 1200: Loss = 0.1453\n",
            "Epoch 4, Batch 1300: Loss = 0.1614\n",
            "Epoch 4, Batch 1400: Loss = 0.1519\n",
            "Epoch 4, Batch 1500: Loss = 0.1330\n",
            "Epoch 4, Batch 1600: Loss = 0.1472\n",
            "Epoch 4, Batch 1700: Loss = 0.1427\n",
            "Epoch 4, Batch 1800: Loss = 0.1346\n",
            "Epoch 5, Batch 100: Loss = 0.1142\n",
            "Epoch 5, Batch 200: Loss = 0.1463\n",
            "Epoch 5, Batch 300: Loss = 0.1109\n",
            "Epoch 5, Batch 400: Loss = 0.1225\n",
            "Epoch 5, Batch 500: Loss = 0.1280\n",
            "Epoch 5, Batch 600: Loss = 0.1405\n",
            "Epoch 5, Batch 700: Loss = 0.1437\n",
            "Epoch 5, Batch 800: Loss = 0.1349\n",
            "Epoch 5, Batch 900: Loss = 0.1268\n",
            "Epoch 5, Batch 1000: Loss = 0.1349\n",
            "Epoch 5, Batch 1100: Loss = 0.1405\n",
            "Epoch 5, Batch 1200: Loss = 0.1218\n",
            "Epoch 5, Batch 1300: Loss = 0.1280\n",
            "Epoch 5, Batch 1400: Loss = 0.1334\n",
            "Epoch 5, Batch 1500: Loss = 0.1220\n",
            "Epoch 5, Batch 1600: Loss = 0.1203\n",
            "Epoch 5, Batch 1700: Loss = 0.1231\n",
            "Epoch 5, Batch 1800: Loss = 0.1190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for Task 2 in PyTorch"
      ],
      "metadata": {
        "id": "yJT62KRBd8Gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "ZE7xNYJfdcz5"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test data (same preprocessing as Task 1)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "test_data = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)"
      ],
      "metadata": {
        "id": "R_sOmmUtdgOx"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuild the model architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n"
      ],
      "metadata": {
        "id": "XWZyIc0mdle4"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model from Task 1\n",
        "model = Net()\n",
        "model.load_state_dict(torch.load('mnist_model.pth'))\n",
        "model.eval()  # Set to evaluation mode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1A_llGGdvgb",
        "outputId": "6a0e9efb-137d-4530-f58a-50c328dc9418"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += pred.eq(target).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {correct / len(test_loader.dataset):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yf8Wns3VdzEz",
        "outputId": "6c21fc28-8b3d-41f1-9121-f46f687cb2cd"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c9e6b62",
        "outputId": "99b77cfd-9a0a-4cd9-9677-fe281a19816d"
      },
      "source": [
        "%pip install onnx"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (1.18.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 3 Conversion to ONNX\n",
        "import onnx\n",
        "dummy_input = torch.randn(1, 1, 28, 28)  # Batch of 1 image\n",
        "torch.onnx.export(model, dummy_input, \"model.onnx\",\n",
        "                  input_names=[\"input\"], output_names=[\"output\"])"
      ],
      "metadata": {
        "id": "cslyTa3ThPmd"
      },
      "execution_count": 78,
      "outputs": []
    }
  ]
}